# -*- coding: utf-8 -*-
"""Tomato Leaf -Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sSoUaMWw6WrZet7vSdtz6EQJdLuxRgkD
"""

# Commented out IPython magic to ensure Python compatibility.
# Import Library
import os, shutil
import zipfile
import random
from random import sample
import shutil
from shutil import copyfile
import pathlib
from pathlib import Path
import numpy as np
import pandas as pd
from tqdm.notebook import tqdm as tq

# Mengimpor libraries untuk visualisasi
# %matplotlib inline
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.image import imread

# Mengimpor libraries untuk pemrosesan data gambar
import cv2
from PIL import Image
import skimage
from skimage import io
from skimage.transform import resize
from skimage.transform import rotate, AffineTransform, warp
from skimage import img_as_ubyte
from skimage.exposure import adjust_gamma
from skimage.util import random_noise

# Mengimpor libraries untuk pembuatan dan evaluasi model
import keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import tensorflow as tf
from tensorflow.keras import Model, layers
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.optimizers import Adam, RMSprop, SGD
from tensorflow.keras.layers import InputLayer, Conv2D, SeparableConv2D, MaxPooling2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.applications import MobileNet
from tensorflow.keras.applications.densenet import DenseNet121
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau

# Mengabaikan peringatan
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# Mencetak versi TensorFlow yang sedang digunakan
print(tf.__version__)

# Import module yang disediakan google colab untuk kebutuhan upload file
from google.colab import files
files.upload()

!rm -rf tomatoleaf/

!pip install kaggle

# Connect kaggle to colab
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download kaggle dataset and unzip the file
!kaggle datasets download -d kaustubhb999/tomatoleaf
!unzip tomatoleaf.zip

import os
import shutil

# Direktori awal untuk train dan test
train_dir = "tomato/train"
test_dir = "tomato/val"

# Direktori baru untuk dataset gabungan
combined_dir = "tomato/dataset"

# Buat direktori baru untuk dataset gabungan
os.makedirs(combined_dir, exist_ok=True)

# Salin file dan folder dari train
for category in os.listdir(train_dir):
    category_dir = os.path.join(train_dir, category)
    if os.path.isdir(category_dir):
        shutil.copytree(category_dir, os.path.join(combined_dir, category), dirs_exist_ok=True)

# Salin file dan folder dari test
for category in os.listdir(test_dir):
    category_dir = os.path.join(test_dir, category)
    if os.path.isdir(category_dir):
        shutil.copytree(category_dir, os.path.join(combined_dir, category), dirs_exist_ok=True)

old_dir = 'tomato/dataset/Tomato___Spider_mites Two-spotted_spider_mite'
new_dir = 'tomato/dataset/Tomato___Spider_mites'

os.rename(old_dir, new_dir)

# Membuat kamus yang menyimpan gambar untuk setiap kelas dalam data
tomato_image = {}

# Tentukan path sumber train
path = "tomato/"
path_sub = os.path.join(path, "dataset")
for i in os.listdir(path_sub):
    tomato_image[i] = os.listdir(os.path.join(path_sub, i))

# Menampilkan secara acak 5 gambar di bawah setiap dari 2 kelas dari data.
# Anda akan melihat gambar yang berbeda setiap kali.
path_sub = "tomato/dataset/"

# Menampilkan secara acak 5 gambar di bawah setiap kelas dari data latih
fig, axs = plt.subplots(len(tomato_image.keys()), 5, figsize=(15, 15))

for i, class_name in enumerate(os.listdir(path_sub)):
    images = np.random.choice(tomato_image[class_name], 5, replace=False)

    for j, image_name in enumerate(images):
        img_path = os.path.join(path_sub, class_name, image_name)
        img = Image.open(img_path)
        axs[i, j].imshow(img, cmap='gray')
        axs[i, j].set(xlabel=class_name, xticks=[], yticks=[])


fig.tight_layout()

# Define source path
tomato_path = "tomato/dataset/"

# Create a list that stores data for each filenames, filepaths, and labels in the data
file_name = []
labels = []
full_path = []

# Get data image filenames, filepaths, labels one by one with looping, and store them as dataframe
for path, subdirs, files in os.walk(tomato_path):
    for name in files:
        full_path.append(os.path.join(path, name))
        labels.append(path.split('/')[-1])
        file_name.append(name)

distribution_train = pd.DataFrame({"path":full_path,'file_name':file_name,"labels":labels})

# Print the count of each category
print(distribution_train['labels'].value_counts())

# Plot the distribution of images across the classes
Label = distribution_train['labels']
plt.figure(figsize = (6,6))
sns.set_style("darkgrid")
plot_data = sns.countplot(Label)

import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
import random

# Membuat fungsi untuk melakukan rotasi berlawanan arah jarum jam
def anticlockwise_rotation(img):
    img = tf.image.resize(img, (224, 224))
    img = tf.image.rot90(img, k=random.randint(1, 4))  # Rotasi 90, 180, atau 270 derajat secara acak
    return img

# Membuat fungsi untuk melakukan rotasi searah jarum jam
def clockwise_rotation(img):
    img = tf.image.resize(img, (224, 224))
    img = tf.image.rot90(img, k=random.randint(1, 4))  # Rotasi 90, 180, atau 270 derajat secara acak
    return img

# Membuat fungsi untuk membalik gambar secara vertikal dari atas ke bawah
def flip_up_down(img):
    img = tf.image.resize(img, (224, 224))
    img = tf.image.flip_up_down(img)
    return img

# Membuat fungsi untuk memberikan efek pergeseran acak pada gambar
def sheared(img):
    img = tf.image.resize(img, (224, 224))
    # Buat objek ImageDataGenerator dengan parameter shearing range
    datagen = ImageDataGenerator(shear_range=0.2)
    img = next(iter(datagen.flow(tf.expand_dims(img, 0))))[0]
    return img

# Membuat fungsi untuk melakukan pergeseran melengkung pada gambar
def warp_shift(img):
    img = tf.image.resize(img, (224, 224))
    # Buat objek ImageDataGenerator dengan parameter width_shift_range dan height_shift_range
    datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1)
    img = next(iter(datagen.flow(tf.expand_dims(img, 0))))[0]
    return img

import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
import random
import os
import cv2
from skimage import io
from skimage import img_as_ubyte

# Define the transformations
transformations = {
    'rotate anticlockwise': anticlockwise_rotation,
    'rotate clockwise': clockwise_rotation,
    'warp shift': warp_shift,
    'flip up down': flip_up_down,
    'shear image': sheared
}

images_paths = [
    ("tomato/dataset/Tomato___Bacterial_spot", "tomato/dataset/Tomato___Bacterial_spot"),
    ("tomato/dataset/Tomato___Early_blight", "tomato/dataset/Tomato___Early_blight"),
    ("tomato/dataset/Tomato___Late_blight", "tomato/dataset/Tomato___Late_blight"),
    ("tomato/dataset/Tomato___Leaf_Mold", "tomato/dataset/Tomato___Leaf_Mold"),
    ("tomato/dataset/Tomato___Septoria_leaf_spot", "tomato/dataset/Tomato___Septoria_leaf_spot"),
    ("tomato/dataset/Tomato___Spider_mites", "tomato/dataset/Tomato___Spider_mites"),
    ("tomato/dataset/Tomato___Target_Spot", "tomato/dataset/Tomato___Target_Spot"),
    ("tomato/dataset/Tomato___Tomato_Yellow_Leaf_Curl_Virus", "tomato/dataset/Tomato___Tomato_Yellow_Leaf_Curl_Virus"),
    ("tomato/dataset/Tomato___Tomato_mosaic_virus", "tomato/dataset/Tomato___Tomato_mosaic_virus"),
    ("tomato/dataset/Tomato___healthy", "tomato/dataset/Tomato___healthy")
]

images_to_generate = [
    (5000, images_paths[0][0], images_paths[0][1]),
    (5000, images_paths[1][0], images_paths[1][1]),
    (5000, images_paths[2][0], images_paths[2][1]),
    (5000, images_paths[3][0], images_paths[3][1]),
    (5000, images_paths[4][0], images_paths[4][1]),
    (5000, images_paths[5][0], images_paths[5][1]),
    (5000, images_paths[6][0], images_paths[6][1]),
    (5000, images_paths[7][0], images_paths[7][1]),
    (5000, images_paths[8][0], images_paths[8][1]),
    (5000, images_paths[9][0], images_paths[9][1])
]

for num_images, original_path, aug_path in images_to_generate:
    i = 1
    while i <= num_images:
        image_path = random.choice(os.listdir(original_path))
        image = os.path.join(original_path, image_path)
        try:
            try:
                original_image = io.imread(image)
            except Exception as e:
                print(f"Error reading image {image}: {e}. Skipping...")
                continue

            original_image = original_image / 255.0  # Normalize image values to be between 0 and 1
            if original_image.min() < -1 or original_image.max() > 1:
                print(f"Image {image} has values outside the range of -1 to 1. Skipping...")
                continue
            original_image = original_image - 1.0  # Shift image values to be between -1 and 1

            # Rest of the code remains the same

            # Check if the image has valid dimensions (3 or 4 dimensions)
            if original_image.ndim not in [3, 4]:
                raise ValueError('Invalid image dimensions')

            transformed_image = None
            n = 0  # Variable to iterate until the specified number of transformations
            transformation_count = random.randint(1, len(transformations))  # Randomly select the number of transformations to apply

            while n <= transformation_count:
                key = random.choice(list(transformations))  # Randomly select and call the transformation method
                transformed_image = transformations[key](original_image)
                n = n + 1

            filename, file_extension = os.path.splitext(image_path)
            new_image_path = os.path.join(aug_path, f"augmented_{filename}{file_extension}")
            transformed_image = transformed_image* 255
            transformed_image = np.array(transformed_image, dtype=np.uint8)
            transformed_image = img_as_ubyte(transformed_image)
            cv2.imwrite(new_image_path, transformed_image)
            print(f"Berhasil")
            i = i + 1
        except ValueError as e:
            print('Could not read or process the image', image, ':', e, 'hence skipping it.')

# Define source path
tomato_path = "tomato/dataset"

# Create a list that stores data for each filenames, filepaths, and labels in the data
file_name = []
labels = []
full_path = []

# Get data image filenames, filepaths, labels one by one with looping, and store them as dataframe
for path, subdirs, files in os.walk(tomato_path):
    for name in files:
        full_path.append(os.path.join(path, name))
        labels.append(path.split('/')[-1])
        file_name.append(name)

distribution_train = pd.DataFrame({"path":full_path,'file_name':file_name,"labels":labels})

# Print the count of each category
print(distribution_train['labels'].value_counts())

# Plot the distribution of images across the classes
Label = distribution_train['labels']
plt.figure(figsize = (6,6))
sns.set_style("darkgrid")
plot_data = sns.countplot(Label)

# Panggil variabel mypath yang menampung folder dataset gambar
mypath= 'tomato/dataset'

file_name = []
labels = []
full_path = []
for path, subdirs, files in os.walk(mypath):
    for name in files:
        full_path.append(os.path.join(path, name))
        labels.append(path.split('/')[-1])
        file_name.append(name)


# Memasukan variabel yang sudah dikumpulkan pada looping di atas menjadi sebuah dataframe agar rapih
df = pd.DataFrame({"path":full_path,'file_name':file_name,"labels":labels})
# Melihat jumlah data gambar pada masing-masing label
df.groupby(['labels']).size()

# Variabel yang digunakan pada pemisahan data ini dimana variabel x = data path dan y = data labels
X= df['path']
y= df['labels']

# Split dataset awal menjadi data train dan test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=300)

# Menyatukan kedalam masing-masing dataframe
df_tr = pd.DataFrame({'path':X_train,'labels':y_train,'set':'train'})
df_te = pd.DataFrame({'path':X_test,'labels':y_test,'set':'test'})

# Print hasil diatas untuk melihat panjang size data training dan testing
print('train size', len(df_tr))
print('test size', len(df_te))

# Gabungkan DataFrame df_tr dan df_te
df_all = pd.concat([df_tr, df_te], ignore_index=True)

print('===================================================== \n')
print(df_all.groupby(['set', 'labels']).size(), '\n')
print('===================================================== \n')

# Cek sample data
print(df_all.sample(5))

# Memanggil dataset asli yang berisi keseluruhan data gambar yang sesuai dengan labelnya
datasource_path = "tomato/dataset/"
# Membuat variabel Dataset, dimana nanti menampung data yang telah dilakukan pembagian data training dan testing
dataset_path = "Dataset-Final/"

for index, row in tq(df_all.iterrows()):
    # Deteksi filepath
    file_path = row['path']
    if os.path.exists(file_path) == False:
            file_path = os.path.join(datasource_path,row['labels'],row['image'].split('.')[0])

    # Buat direktori tujuan folder
    if os.path.exists(os.path.join(dataset_path,row['set'],row['labels'])) == False:
        os.makedirs(os.path.join(dataset_path,row['set'],row['labels']))

    # Tentukan tujuan file
    destination_file_name = file_path.split('/')[-1]
    file_dest = os.path.join(dataset_path,row['set'],row['labels'],destination_file_name)

    # Salin file dari sumber ke tujuan
    if os.path.exists(file_dest) == False:
        shutil.copy2(file_path,file_dest)

# Define training and test directories
TRAIN_DIR = "Dataset-Final/train/"
TEST_DIR = "Dataset-Final/test/"

train_Tomato___Bacterial_spot = os.path.join(TRAIN_DIR + '/Tomato___Bacterial_spot')
train_Tomato___Early_blight = os.path.join(TRAIN_DIR + '/Tomato___Early_blight')
train_Tomato___Late_blight = os.path.join(TRAIN_DIR + '/Tomato___Late_blight')
train_Tomato___Leaf_Mold = os.path.join(TRAIN_DIR + '/Tomato___Leaf_Mold')
train_Tomato___Septoria_leaf_spot = os.path.join(TRAIN_DIR + '/Tomato___Septoria_leaf_spot')
train_Tomato___Spider_mites = os.path.join(TRAIN_DIR + '/Tomato___Spider_mites')
train_Tomato___Target_Spot = os.path.join(TRAIN_DIR + '/Tomato___Target_Spot')
train_Tomato___Tomato_Yellow_Leaf_Curl_Virus = os.path.join(TRAIN_DIR + '/Tomato___Tomato_Yellow_Leaf_Curl_Virus')
train_Tomato___Tomato_mosaic_virus = os.path.join(TRAIN_DIR + '/Tomato___Tomato_mosaic_virus')
train_Tomato___healthy = os.path.join(TRAIN_DIR + '/Tomato___healthy')

test_Tomato___Bacterial_spot = os.path.join(TEST_DIR + '/Tomato___Bacterial_spot')
test_Tomato___Early_blight = os.path.join(TEST_DIR + '/Tomato___Early_blight')
test_Tomato___Late_blight = os.path.join(TEST_DIR + '/Tomato___Late_blight')
test_Tomato___Leaf_Mold = os.path.join(TEST_DIR + '/Tomato___Leaf_Mold')
test_Tomato___Septoria_leaf_spot = os.path.join(TEST_DIR + '/Tomato___Septoria_leaf_spot')
test_Tomato___Spider_mites = os.path.join(TEST_DIR + '/Tomato___Spider_mites')
test_Tomato___Target_Spot = os.path.join(TEST_DIR + '/Tomato___Target_Spot')
test_Tomato___Tomato_Yellow_Leaf_Curl_Virus = os.path.join(TEST_DIR + '/Tomato___Tomato_Yellow_Leaf_Curl_Virus')
test_Tomato___Tomato_mosaic_virus = os.path.join(TEST_DIR + '/Tomato___Tomato_mosaic_virus')
test_Tomato___healthy = os.path.join(TEST_DIR + '/Tomato___healthy')

print("Total number of Tomato___Bacterial_spot images in training set: ",len(os.listdir(train_Tomato___Bacterial_spot)))
print("Total number of Tomato___Early_blight images in training set: ",len(os.listdir(train_Tomato___Early_blight)))
print("Total number of Tomato___Late_blight images in training set: ",len(os.listdir(train_Tomato___Late_blight)))
print("Total number of Tomato___Leaf_Mold images in training set: ",len(os.listdir(train_Tomato___Leaf_Mold)))
print("Total number of Tomato___Septoria_leaf_spot images in training set: ",len(os.listdir(train_Tomato___Septoria_leaf_spot)))
print("Total number of Tomato___Spider_mites images in training set: ",len(os.listdir(train_Tomato___Spider_mites)))
print("Total number of Tomato___Target_Spot images in training set: ",len(os.listdir(train_Tomato___Target_Spot)))
print("Total number of Tomato___Tomato_Yellow_Leaf_Curl_Virus images in training set: ",len(os.listdir(train_Tomato___Tomato_Yellow_Leaf_Curl_Virus)))
print("Total number of Tomato___Tomato_mosaic_virus images in training set: ",len(os.listdir(train_Tomato___Tomato_mosaic_virus)))
print("Total number of Tomato___healthy images in training set: ",len(os.listdir(train_Tomato___healthy)))
print("\n")
print("Total number of Tomato___Bacterial_spot images in test set: ",len(os.listdir(test_Tomato___Bacterial_spot)))
print("Total number of Tomato___Early_blight images in test set: ",len(os.listdir(test_Tomato___Early_blight)))
print("Total number of Tomato___Late_blight images in test set: ",len(os.listdir(test_Tomato___Late_blight)))
print("Total number of Tomato___Leaf_Mold images in test set: ",len(os.listdir(test_Tomato___Leaf_Mold)))
print("Total number of Tomato___Septoria_leaf_spot images in test set: ",len(os.listdir(test_Tomato___Septoria_leaf_spot)))
print("Total number of Tomato___Spider_mites images in test set: ",len(os.listdir(test_Tomato___Spider_mites)))
print("Total number of Tomato___Target_Spot images in test set: ",len(os.listdir(test_Tomato___Target_Spot)))
print("Total number of Tomato___Tomato_Yellow_Leaf_Curl_Virus images in test set: ",len(os.listdir(test_Tomato___Tomato_Yellow_Leaf_Curl_Virus)))
print("Total number of Tomato___Tomato_mosaic_virus images in test set: ",len(os.listdir(test_Tomato___Tomato_mosaic_virus)))
print("Total number of Tomato___healthy images in test set: ",len(os.listdir(test_Tomato___healthy)))

import os
from tensorflow.keras.preprocessing.image import ImageDataGenerator

TRAIN_DIR = "Dataset-Final/train/"
TEST_DIR = "Dataset-Final/test/"

train_dirs = [f for f in os.listdir(TRAIN_DIR) if not f.startswith('.')]

datagen = ImageDataGenerator(rescale=1./255)

train_datagen = ImageDataGenerator(rescale=1./255)
validation_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=(128, 128),
    batch_size=32,
    class_mode='categorical',
    classes=train_dirs,
    shuffle=True
)

validation_generator = validation_datagen.flow_from_directory(
    TEST_DIR,
    target_size=(128, 128),
    batch_size=32,
    class_mode='categorical',
    classes=train_dirs,
    shuffle=True
)

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.callbacks import EarlyStopping

# Buat model
model = Sequential()

# Tambahkan lapisan konvolusi dan pooling
model.add(Conv2D(256, (3, 3), activation='relu', input_shape=(128, 128, 3)))
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))

# Tambahkan lapisan flatten
model.add(Flatten())

# Tambahkan lapisan dense
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))  # Tambah Dropout layer dengan tingkat dropout 0.5

# Tambahkan lapisan output
model.add(Dense(10, activation='softmax'))

# Kompilasi model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Definisikan early stopping
early_stopping = EarlyStopping(monitor='val_accuracy', patience=25, min_delta=0.01, mode='max', restore_best_weights=True)

print(model.summary())

history_1 = model.fit(train_generator,
                        epochs=50,
                        validation_data=validation_generator,
                        callbacks=[early_stopping])

acc = history_1.history['accuracy']
val_acc = history_1.history['val_accuracy']
loss = history_1.history['loss']
val_loss = history_1.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r')
plt.plot(epochs, val_acc, 'b')
plt.title('Training and Validation Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

plt.plot(epochs, loss, 'r')
plt.plot(epochs, val_loss, 'b')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.title('Training and Validaion Loss')
plt.show()

from google.colab import files
import tensorflow as tf
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

# Upload an image manually
uploaded = files.upload()

# Get the uploaded image file
img_file = list(uploaded.keys())[0]

# Open the image file
img = Image.open(img_file)

# Resize the image to match the model's input shape
img = img.resize((128, 128))

# Convert the image to a numpy array
img_array = np.array(img)

# Normalize the image
img_array = img_array / 255.0

# Add a batch dimension
img_array = np.expand_dims(img_array, 0)

# Make predictions
predictions = model.predict(img_array)

# Get the predicted class
predicted_class = np.argmax(predictions, axis=1)

# Define the class labels
class_labels = ['Bacterial Spot', 'Early Blight', 'Late Blight', 'Leaf Mold', 'Septoria Leaf Spot', 'Spider Mites', 'Target Spot', 'Yellow Leaf Curl Virus', 'Mosaic Virus', 'Healty']

# Get the class label
predicted_label = class_labels[predicted_class[0]]

# Display the uploaded image without axes
plt.imshow(img)
plt.axis('off')
plt.show()

print("Predicted label (desease):", predicted_label)

model.save("model.h5")

# Install tensorflowjs
!pip install tensorflowjs

# Convert model.h5 to model
!tensorflowjs_converter --input_format=keras model.h5 tfjs_model

import tensorflow as tf

# Load model .h5
model = tf.keras.models.load_model("model.h5")

# Konversi model ke .tflite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save model .tflite
with open("model.tflite", "wb") as f:
    f.write(tflite_model)

save_path = os.path.join("models/rps_model/1/")
tf.saved_model.save(model, save_path)

!pip freeze requirements.txt